\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

% It's fine to compress itemized lists if you used them in the
% manuscript
\usepackage{enumitem}
\setlist{nosep, leftmargin=14pt}

\usepackage{mwe} % to get dummy images

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{A Dual-Task Study: Life Expectancy Prediction and Sentiment Analysis of Film Reviews}
%
% Single address.
% ---------------
\name{Zili Gong, Jihan Li, Chunlin Wang, Qijun Han}
\address{
  School of Automation and Intelligent Manufacturing, Southern University of Science and Technology\\
  Shenzhen, Guangdong, China\\
  \{gongzl2022, lijh2022, wangcl2022, hanqj2022\}@mail.sustech.edu.cn
}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
This paper presents a comprehensive study across two distinct machine learning domains. The first task focuses on predictive modeling, where we develop a model to forecast national life expectancy using a range of socio-economic and environmental indicators from 2008 to 2018. We explore various regression techniques, feature importance, and model improvement strategies. The second task delves into natural language processing, conducting sentiment analysis on Douban movie reviews. We implement and compare traditional machine learning classifiers with modern Large Language Model (LLM) approaches, evaluating their effectiveness in discerning positive from negative sentiment in textual data. This work highlights the application of diverse statistical methods to solve real-world prediction and classification problems.
\end{abstract}
%
\begin{keywords}
Life Expectancy, Predictive Modeling, Sentiment Analysis, Machine Learning, Natural Language Processing, LLM
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

This report details our work on two data science projects. 
The first project, "Life Expectancy," involves predicting life expectancy at birth based on 12 features for 211 countries. 
The primary objective is to train a model on data from 2008-2017 to predict life expectancy for the year 2018, 
using the $\textit{life\_indicator\_2008-2018}$ dataset.

The second project, "Douban Movie Comment Analysis," aims to classify the sentiment of movie reviews from Douban as either positive or negative. This task utilizes the $\textit{douban\_movie}$ dataset. We explore both traditional machine learning techniques and the capabilities of Large Language Models (LLMs) for this text classification problem.

\section{Task 1: Life Expectancy Prediction}
\label{sec:task1}

The goal of this task is to build a regression model to predict `Life expectancy at birth` using various national indicators.

\subsection{Data Understanding}
\label{ssec:data_understanding}

The dataset contains 12 features, including `Agriculture, forestry, and fishing, value added (% of GDP)`, `GDP (current US$)`, and `Current health expenditure (% of GDP)`. We hypothesized that features related to health expenditure, immunization rates, and GDP would have a significant positive impact on life expectancy, while a high prevalence of underweight children would have a negative impact.

A correlation heatmap was generated to visualize the relationships between features. Missing data was a significant issue, and we compared several imputation methods, including mean/median filling, interpolation, and K-Nearest Neighbors (KNN) imputation.

\textbf{TODO:} Insert the correlation heatmap figure. Discuss which imputation method was chosen and why, based on performance comparisons.

\subsection{Modeling}
\label{ssec:modeling_life}

We trained and evaluated several regression models to identify the best predictor for life expectancy. The models included Linear Regression, Lasso, Ridge, Random Forest, XGBoost, and Support Vector Regression (SVR). The data from 2008 to 2017 served as the training set, and the 2018 data was used for testing.

Model performance was evaluated using Mean Squared Error (MSE) and the coefficient of determination ($R^2$).

\textbf{TODO:} Present a table comparing the MSE and $R^2$ scores for each model on the 2018 test set. Analyze the results and select the best-performing model.

Feature importance was extracted from the best models (e.g., coefficients from linear models, feature importance scores from tree-based models) to identify the key drivers of life expectancy.

\textbf{TODO:} List the top 5 most important features and discuss whether they align with the initial hypotheses.

\subsection{Analysis of Predictions}
\label{ssec:analysis_preds}

We visualized the residuals (the difference between predicted and actual values) for the 2018 data to assess the model's accuracy. Outliers, i.e., countries where the prediction error was particularly large, were identified.

\textbf{TODO:} Include a plot of predicted vs. actual values for 2018. Identify any major outliers and provide potential explanations for the large prediction errors (e.g., unique socio-economic events in those countries in 2018). Analyze the distribution of prediction errors.

\subsection{Model Improvement}
\label{ssec:model_improvement}

To enhance model performance, we employed stepwise forward selection to find an optimal subset of features. Additionally, we engineered new features, such as `GDP per capita` (GDP / Population), to better capture the economic status of a country.

\textbf{TODO:} Describe the results of the model improvement techniques. Did stepwise selection or feature engineering lead to a significant improvement in MSE or $R^2$?

\section{Task 2: Douban Movie Comment Analysis}
\label{sec:task2}

This task focuses on binary sentiment classification of movie reviews. Reviews with star ratings of 1 or 2 were labeled as negative, while those with ratings of 3, 4, or 5 were labeled as positive.

\subsection{Part 1: Machine Learning Approach}
\label{ssec:ml_approach}

\subsubsection{Text Preprocessing}
\label{sssec:preprocessing}
The raw text comments were preprocessed to prepare them for vectorization. This involved tokenization (using a Chinese tokenizer like Jieba), removal of stopwords, special symbols, and low-frequency words.

\subsubsection{Text Vectorization}
\label{sssec:vectorization}
We converted the cleaned text into numerical vectors using TF-IDF.

\textbf{TODO (Bonus):} If Word2Vec or BERT embeddings were used, describe the process and compare the results with TF-IDF.

\subsubsection{Model Training \& Evaluation}
\label{sssec:ml_training}
We trained and cross-validated Logistic Regression and Naive Bayes classifiers on an 80/20 train/test split of the data. Performance was measured using accuracy, precision, recall, and F1-score.

\textbf{TODO:} Present a table with the evaluation metrics for both models. Analyze their performance and discuss their respective strengths and weaknesses for this task.

\subsection{Part 2: Large Language Model (LLM) Approach}
\label{ssec:llm_approach}

\subsubsection{Prompt Design \& In-Context Learning}
\label{sssec:prompt_design}
To leverage LLMs for this task, we designed effective prompts. We experimented with zero-shot and few-shot learning. For few-shot learning, the prompt included examples of positive and negative reviews to guide the model. An example of a few-shot prompt structure is:

\begin{verbatim}
Based on the examples, determine the sentiment of the final review.

1. Comment: The acting was superb, and the plot was engaging.
   Sentiment: Positive

2. Comment: The special effects were terrible and the story was boring.
   Sentiment: Negative

Now analyze:
Comment: {{review_text}}
Sentiment:
\end{verbatim}

\subsubsection{LLM API Testing}
\label{sssec:llm_api}
We used the APIs for two LLMs (e.g., ChatGPT-3.5, DeepSeek) to predict the sentiment of a sample of reviews from our test set.

\textbf{TODO:} Name the specific LLMs used.

\subsubsection{Discussion}
\label{sssec:discussion}
We compared the performance of the traditional machine learning models with the LLM-based predictions.

\textbf{TODO:} Compare the accuracy of the LLMs against the Logistic Regression and Naive Bayes models. Discuss the strengths and limitations of each approach. Analyze specific cases where the ML and LLM predictions differed and provide insights into why.

\section{Bonus Tasks}
\label{sec:bonus}

\subsection{Task 1 Bonus: Forecasting to 2025}
A key challenge explored was the feasibility of forecasting life expectancy for 2025. This requires extrapolating the feature trends from 2008-2018 and feeding them into the trained regression model.

\textbf{TODO:} Discuss the methodology used for feature extrapolation (e.g., time series forecasting on each feature) and present the 2025 life expectancy predictions. Analyze the confidence and potential error sources of this long-range forecast.

\subsection{Task 2 Bonus: Advanced NLP Exploration}

\subsubsection{Advanced Data Analysis}
We conducted further analysis on the review data. This included generating word clouds to visualize the most frequent terms in positive and negative reviews and exploring the correlation between review length and the assigned star rating.

\textbf{TODO:} Insert word cloud visualizations and a plot showing the relationship between review length and rating. Discuss any insights gained.

\subsubsection{LLM Prompt Optimization}
To improve LLM performance, we tested more sophisticated prompting strategies, such as chain-of-thought and role-playing prompts, analyzing how changes in prompt phrasing affected the quality of sentiment prediction.

\textbf{TODO:} Provide examples of the advanced prompts used and compare their performance to the initial few-shot prompts.

\subsubsection{Fine-Tuning LLMs}
We fine-tuned an open-source LLM (e.g., LLaMA-2) on a small subset of the Douban review data. The performance of the fine-tuned model was then compared against its pre-trained counterpart to evaluate the effectiveness of domain-specific adaptation.

\textbf{TODO:} Describe the fine-tuning process and present a comparison of the model's performance before and after fine-tuning.


\section{Compliance with ethical standards}
\label{sec:ethics}
This research study \cite{LeCun98} was conducted using publicly available data. The life expectancy data is aggregated at a country level, and the movie review data is anonymized. Therefore, no formal ethics approval was required for this study.

\section{Acknowledgments}
\label{sec:acknowledgments}
No external funding was received for conducting this study. The authors have no relevant financial or non-financial interests to disclose.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
% \bibliography{strings,refs}
\begin{thebibliography}{99}

  \bibitem{LeCun98}
  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,
  \newblock ``Gradient-based learning applied to document recognition,''
  \newblock {\em Proceedings of the IEEE}, vol. 86, no. 11, pp. 2278--2324, 1998.
  
  \bibitem{Maaten08}
  L. van der Maaten and G. Hinton,
  \newblock ``Visualizing data using t-SNE,''
  \newblock {\em Journal of Machine Learning Research}, vol. 9, pp. 2579--2605, 2008.
  
  \end{thebibliography}

\end{document}
